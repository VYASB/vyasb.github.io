<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Exploring Bayesian Neural Networks with Pyro | Bharat Vyas</title> <meta name="author" content="Bharat Vyas"> <meta name="description" content="notebook on BNN, part of UvA Deep Learning II course"> <meta name="keywords" content="academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/tree.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://vyasb.github.io/blog/2022/bayesian-nn/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Bharat </span>Vyas</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Exploring Bayesian Neural Networks with Pyro</h1> <p class="post-meta">March 5, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/bayesian"> <i class="fas fa-hashtag fa-sm"></i> bayesian</a>   </p> </header> <article class="post-content"> <h3 id="comparing-bnns-to-non-bayesian-methods-for-uncertainty-estimates">Comparing BNNs to Non-Bayesian Methods for Uncertainty Estimates</h3> <p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Complete_DLII_BNN_2_2.ipynb" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" alt="View on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Complete_DLII_BNN_2_2.ipynb" rel="external nofollow noopener" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Collab"></a> <br> <strong>Empty notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Student_DLII_BNN_2_2.ipynb" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" alt="View Empty on Github"></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Student_DLII_BNN_2_2.ipynb" rel="external nofollow noopener" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open Empty In Collab"></a> <br> <strong>Authors:</strong> Ilze Amanda Auzina, Leonard Bereska and Eric Nalisnick</p> <p>In this tutorial we will investigate what are some benefits of Bayesian Neural Networks (BNN) over point estimate Neural Networks.</p> <p>Import the usual suspects…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">trange</span><span class="p">,</span> <span class="n">tqdm</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">81</span><span class="p">);</span>
</code></pre></div></div> <h3 id="simulate-data">Simulate Data</h3> <p>Let’s simulate a wiggly line and draw observations in separated regions…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set Numpy seed
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># sample observations
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(.</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)])</span>
<span class="n">ε</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ε</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ε</span><span class="p">))</span> <span class="o">+</span> <span class="n">ε</span>

<span class="c1"># compute true function
</span><span class="n">x_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">x_true</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_true</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_true</span><span class="p">)</span>
 


<span class="c1"># make plot
</span><span class="n">xlims</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="n">ylims</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_7_0.png" alt="png"></p> <p>As you can see, we have the true function in blue. The observations are observable in two regions of the function and there is some noise in their measurement. We will use this simple data to showcase the differences between BNN and deterministic NN.</p> <h3 id="define-non-bayesian-neural-network">Define non-Bayesian Neural Network</h3> <p>First let’s create our point estimate neural network, in other words a standard fully connected MLP. We will define the number of hidden layers dynamically so we can reuse the same class for different depths. We will also add two flags, <em>residual</em> and <em>dropout</em> flag, this will allow us to easily use the same architecture for our BNN.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout_prob</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">in_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">out_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">hid_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">n_hid_layers</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span> <span class="c1"># could also be nn.ReLU()
</span>    <span class="n">self</span><span class="p">.</span><span class="n">residual_flag</span> <span class="o">=</span> <span class="n">residual_flag</span>
    <span class="n">self</span><span class="p">.</span><span class="n">dropout_flag</span> <span class="o">=</span> <span class="n">dropout_flag</span>

    <span class="c1"># optional: activate dropout
</span>    <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>
        
    <span class="c1"># dynamically define architecture
</span>    <span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_hid_layers</span> <span class="o">*</span> <span class="p">[</span><span class="n">hid_dim</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">out_dim</span><span class="p">]</span>
    <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span><span class="p">))]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span><span class="n">layer_list</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1">#x = x.reshape(-1, 1)
</span>    <span class="c1"># input --&gt; hid 
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
      <span class="c1"># hid --&gt; hid
</span>      <span class="n">x_temp</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

      <span class="c1"># optional: compute dropout mask
</span>      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout_flag</span><span class="p">:</span>
        <span class="n">x_temp</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x_temp</span><span class="p">)</span>

      <span class="c1"># optional: compute residual connection
</span>      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">residual_flag</span><span class="p">:</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_temp</span> <span class="o">+</span> <span class="n">x</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_temp</span>


    <span class="c1"># hid --&gt; output mean
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu</span>

</code></pre></div></div> <h3 id="train-one-deterministic-nn">Train one deterministic NN</h3> <p><strong>Training</strong></p> <p>Now let’s train our MLP with the training data we generated above:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define model and data
</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">hid_dim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">float</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">]</span> 
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#select an optimizer and loss function
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="c1"># training loop
</span><span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
  <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Evaluate</strong></p> <p>Let’s investigate how our deterministic MLP generalizes over the entire domain of our input variable $x$ (the model was only trained on the observations, now we will also pass in data outside this region)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute predictions everywhere
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">xlims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3000</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> 
<span class="n">y_preds</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">();</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">mlp function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_18_0.png" alt="png"></p> <p>We can see that our deterministic MLP (red line) has correctly learned the data distribution in the training regions, however, as the model has not learned the underlying sinusoidal wave function, it’s predictions outside the training region are inaccurate. As our MLP is a point estimate NN we have no measure confidence in the predictions outside the training region. In the upcoming sections let’s see how this compares to BNN.</p> <h3 id="deep-ensemble">Deep Ensemble</h3> <p>Deep ensembles were first introduced by <a href="https://arxiv.org/abs/1612.01474" rel="external nofollow noopener" target="_blank">Lakshminarayanan et al. (2017)</a>. As the name implies multiple point estimate NN are trained, <em>an ensemble</em>, and the final prediction is computed as an average across the models. From a Bayesian perspective the different point estimates correspond to modes of a Bayesian posterior. This can be interpreted as approximating the posterior with a distribution parametrized as multiple Dirac deltas:</p> <p>\(q_{\phi}(\theta | D) = \sum_{\theta_{i} ∈ ϕ} \alpha_{\theta_{i}} δ_{\theta_{i}}(\theta)\) where $\alpha_{\theta_{i}}$ are positive constants such that their sum is equal to one.</p> <p><strong>Training</strong></p> <p>We will reuse the MLP architecture introduced before, simply now we will train an ensemble of such models</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#define the size of the ensemble: number of NN to train
</span><span class="n">ensemble_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mlp_optimizers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># initialize the ensemble
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">hid_dim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">ensemble</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">mlp_optimizers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train each ensemble component
</span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>

  <span class="c1"># training loop for one NN
</span>  <span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
    <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="nf">net</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Evaluate</strong></p> <p>Same as before, let’s investigate how our Deep Ensemble performs on the entire data domain of our input variable $x$.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute predictions for each network
</span><span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
  <span class="n">y_preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">net</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> <span class="p">)</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_preds</span><span class="p">)</span>
</code></pre></div></div> <p>Plot each ensemble member’s predictive function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot each network's function
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_28_0.png" alt="png"></p> <p>In this plot the benefit of an ensemble approach is not immediately clear. Still on the regions outside the training data each of the trained NN is inaccurate. So what is the benefit you might ask.</p> <p>Well let’s plot the above in a slightly different way: let’s visualize the ensemble’s <strong>uncertainty bands</strong>.</p> <blockquote> <table> <tbody> <tr> <td>From a Bayesian perspective we want to quantity the model’s uncertainty on its prediction. This is done via the marginal $p(y</td> <td>x, D)$, which can be computed as:</td> </tr> </tbody> </table> </blockquote> \[p(y|x, D) = \int_{\theta}p(y|x,\theta')p(\theta'|D)d\theta'\] <blockquote> <table> <tbody> <tr> <td>In practice, for Deep Ensembles we approximate the above by computing the mean and standard deviation across the ensemble. Meaning $p(\theta</td> <td>D)$ represents the parameters of one of the trained models, $\theta_{i} ∼ p(\theta</td> <td>D)$, which we then use to compute $y_{i} = f(x,\theta_{i})$, representing $p(y</td> <td>x,\theta’)$.</td> </tr> </tbody> </table> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute mean and standard deviation
</span><span class="n">y_mean</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">#408765</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">predictive mean</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">y_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#86cfac</span><span class="sh">'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_33_0.png" alt="png"></p> <p>Now we see the benefit of a Bayesian approach. Outside the training region we not only have the point estimate, but also model’s uncertainty about it’s predicition.</p> <h3 id="monte-carlo-dropout">Monte Carlo Dropout</h3> <p>First we create our MC-Dropout Network. As you can see in the code below, creating a dropout network is extremely simple:</p> <blockquote> <p>we can reuse our existing network architecture, the only alteration is that during the forward pass we randomly <em>switch off</em> (zero) some of the elements of the input tensor</p> </blockquote> <table> <tbody> <tr> <td>The Bayesian interpretation of MC-Dropout is that we can see each dropout configuration as a different sample from the approximate posterior distribution $\theta_{i} ∼ q(\theta</td> <td>D)$.</td> </tr> </tbody> </table> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define MLP with dropout (probability of 0.5)
</span><span class="n">dropout_mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">hid_dim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training 
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
  <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Evaluate</strong></p> <p>Similarly to Deep Ensembles, we pass the test data multiple times through the MC-Dropout network. We do so to obtain $y_{i}$ at the different parameter settings, $\theta_{i}$ of the network, $y_{i}=f(x,\theta_{i})$, governed by the dropout mask.</p> <blockquote> <p>This is the main difference compared to dropout implementation in a deterministic NN where it serves as a regularization term. In normal dropout application during test time the dropout is <strong>not</strong> applied. Meaning that all connections are present, but the weights are <a href="https://cs231n.github.io/neural-networks-2/" rel="external nofollow noopener" target="_blank">adjusted</a></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_mc_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># compute predictions, resampling dropout mask for each forward pass
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_mc_samples</span><span class="p">):</span>
  <span class="n">y_preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> <span class="p">)</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_preds</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot of each dropout (sub)-model
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot each network's function
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_43_0.png" alt="png"></p> <p>In the above plot each colored line (apart from blue) represents a different parametrization, $\theta_{i}$, of our MC-Dropout Network.</p> <p>Likewise to the Deep Ensemble Network, we can also compute the MC-dropout’s <strong>uncertainty bands</strong>.</p> <blockquote> <p>The approach in practice is the same as before: we compute the mean and standard deviation across each dropout mask, which corresponds to the marginal estimation we discussed earlier.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute mean and standard deviation
</span><span class="n">y_mean</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">#408765</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">predictive mean</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">y_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#86cfac</span><span class="sh">'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_47_0.png" alt="png"></p> <p>In the same way as Deep Ensembles, MC-Dropout allows us to have an uncertainty estimate next to our point wise predictions. However, for the given use-case this has come with the cost of an overall drop in the model’s performance on the training regions. We observe this because at every pass through our network we randomly choose which nodes to keep, so one could argue that we hinder the networks optimal performance.</p> <h2 id="exercise-detecting-distribution-shift-on-mnist">Exercise: Detecting Distribution Shift on MNIST</h2> <p>In this exercise we will compare Bayesian NNs with deterministic NN on distribution shift detection task. To do this, we’ll monitor the predictive entropy as the distribution gradually shifts. A model with better uncertainty quantification should become less certain—that is, have a more entropic predictive distribuiton—as the input distribution shifts. Mathematically, our quantity of interest is: \(\mathbb{H}[y | x^{*}, D] = - \sum_{y} p(y | x^{*}, D) \log p(y | x^{*}, D)\) where $p(y | x^{*}, D)$ is the predictive distribuiton: \(p(y | x^{*}, D) = \int_{\theta} p(y | x^{*}, \theta) \ p(\theta | D) \ d \theta.\) The goal is to essentially replicate Figure #1 from the paper <a href="https://arxiv.org/abs/1603.04733" rel="external nofollow noopener" target="_blank">Multiplicative Normalizing Flows for Variational Bayesian Neural Networks</a>, comparing MC dropout, ensembles, and a Bayesian NN.</p> <p>We will be using the MNIST dataset, a set of 70,000 hand-written digit images, and we will generate a gradual distribution shift on the dataset by rotating the images. As such, the final plot will depict the change in the entropy of the predictive distribution (y-axis) as degree of rotation increases (x-axis). The paper above shows the result for one image. We, on the other hand, will average over multiple images to make a better comparison between models.</p> <p>First, let’s load MNIST…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># download MNIST
</span><span class="n">mnist_train</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">../data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">../data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz



  0%|          | 0/9912422 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz



  0%|          | 0/28881 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz



  0%|          | 0/1648877 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz



  0%|          | 0/4542 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw
</code></pre></div></div> <p>Visualize an image…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">img_idx</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">255</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_54_0.png" alt="png"></p> <p>We’ll use rotation to simulate a smooth shift. Here’s how you can rotate a given image…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torchvision.transforms.functional</span> <span class="k">as</span> <span class="n">TF</span>

<span class="n">rotation_angle</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">rotated_image</span> <span class="o">=</span> <span class="n">TF</span><span class="p">.</span><span class="nf">rotate</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">img_idx</span><span class="p">][</span><span class="bp">None</span><span class="p">],</span> <span class="n">rotation_angle</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">rotated_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">255</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_56_0.png" alt="png"></p> <p>Let’s setup the training data…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">.</span><span class="n">targets</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">.</span><span class="n">targets</span>
</code></pre></div></div> <p>Now that we have the data, let’s start training neural networks.</p> <h3 id="deterministic-network">Deterministic Network</h3> <p>We will reuse our MLP network architecture but increase the hidden layer dimensionality as well as the number of hidden layers:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training 
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">]))</span>
    <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span> <span class="c1">#x.shape[0]
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Test</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
  <span class="nf">return </span><span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="n">predictions</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">targets</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get test accuracy, to make sure we have a satisfactory model
</span><span class="n">mlp</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">test_preds</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test accuracy is 96.49%
</code></pre></div></div> <h3 id="rotating-the-images">Rotating the images</h3> <p>Now let’s compute predictive entropy on some rotated images…</p> <p>First we will generate the rotated images with an increasing rotation angle from the test images:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use angles from 0 to 90 degrees
</span><span class="n">rotation_angles</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">31</span><span class="p">)]</span>
<span class="n">n_test_images</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">rotated_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">angle</span> <span class="ow">in</span> <span class="n">rotation_angles</span><span class="p">:</span>
  <span class="n">rotated_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">TF</span><span class="p">.</span><span class="nf">rotate</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">.</span><span class="n">data</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">].</span><span class="nf">float</span><span class="p">()[</span><span class="bp">None</span><span class="p">],</span> <span class="n">angle</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_test_images</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div> <p>Evaluate the trained MLP on the rotated images:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">deter_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="n">deter_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">mlp</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
  <span class="nf">return </span><span class="p">(</span><span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Predictive Entropy</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_75_0.png" alt="png"></p> <p>In the above plot we can see how well our deterministic MLP performs on the test images as the rotation angle is increased (increasing distribution shift). As expected, with a higher rotation angle model’s accuracy drops and its predictive entropy increases, as the network class predictions are more uniformly distributed (recall the predictive entropy equation)</p> <h3 id="monte-carlo-dropout-network">Monte Carlo Dropout Network</h3> <p>Let’s create our Dropout Network. We keep the network depth and hidden layer size the same as for the MLP for a fair model comparison</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dropout_mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training 
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">]))</span>
    <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Test</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get test accuracy
</span><span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">test_preds</span> <span class="o">=</span> <span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test accuracy is 96.57%
</code></pre></div></div> <p><strong>Evaluate on rotated images</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_mc_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span> <span class="c1">#we set the model to train so the dropout layer is 'active'
</span>
<span class="n">dropout_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="c1"># compute predictions, resampling dropout mask for each forward pass
</span>  <span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_test_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_mc_samples</span><span class="p">):</span>
    <span class="n">y_preds</span> <span class="o">+=</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">dropout_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">y_preds</span> <span class="o">/</span> <span class="n">n_mc_samples</span> <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot MC dropout results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, MC Dropout</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, MC Dropout</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_86_0.png" alt="png"></p> <p>The accuracy is on par between the MC-Dropout and MLP. However, now we can see that the MC-Dropout has a better uncertainty quantification as it’s predictive entropy is higher. What this means is that overall both model prediction uncertainty increases as the distribution shift increases, however, as MC-dropout computes a model average it is more sensitive (more uncertain) about the predictions with a higher rotation angle.</p> <h3 id="deep-ensemble-1">Deep Ensemble</h3> <p>Now let’s investigate Deep Ensemble performance. We will use the exact same network hyperparameters as for the MLP:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define the size of the ensemble: number of NN to train
</span><span class="n">ensemble_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mlp_optimizers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># initialize the ensemble
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">ensemble</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">mlp_optimizers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train each ensemble component
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
  <span class="c1"># training loop for one NN
</span>  <span class="n">net</span> <span class="o">=</span> <span class="n">ensemble</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
      <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
      <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="nf">net</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">]))</span>
      <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
      <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
      <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">step</span><span class="p">()</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Test</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get test accuracy
# compute predictions for each network
</span><span class="n">test_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
  <span class="n">net</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
  <span class="n">test_preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">net</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">test_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">test_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">test_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">test_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="n">ensemble_size</span> <span class="c1">#we average the predictions over the ensemble
</span>
<span class="c1"># compute the accuracy
</span><span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">.</span><span class="nf">int</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test accuracy is 93.88%
</code></pre></div></div> <p><strong>Evaluate on rotated images</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ensemble_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="c1"># compute predictions for each ensemble network
</span>  <span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_test_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
    <span class="n">y_preds</span> <span class="o">+=</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">net</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ensemble_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">y_preds</span> <span class="o">/</span> <span class="n">ensemble_size</span> <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot Ensemble results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_97_0.png" alt="png"></p> <table> <tbody> <tr> <td>The accuracy of the Deep Ensemble is initially lower than of our deterministic MLP. However, this is accounted by the fact that the Deep Ensemble is better in uncertainty quantification as it’s predictive entropy is higher. How to interpret this? Well, in Deep Ensemble we have trained multiple deterministic MLPs. As the rotation angle increases the point-estimates of each model become more varied, hence the average of these point estimates, $p(y</td> <td>x^{*},D)$, has a more uniform distribution across the classes. As such, the resulting predictive entorpy is higher, as the log of $p(y</td> <td>x^{*},D)$ is more negative for smaller values (recall <a href="https://upload.wikimedia.org/wikipedia/commons/8/81/Logarithm_plots.png" rel="external nofollow noopener" target="_blank">log function</a>).</td> </tr> </tbody> </table> <h3 id="bayesian-neural-network">Bayesian Neural Network</h3> <p>We will train a <em>truly</em> Bayesian Neural Netowrk with SVI</p> <p>First, lets install the pyro package:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">pyro</span><span class="o">-</span><span class="n">ppl</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Collecting pyro-ppl
  Downloading pyro_ppl-1.8.1-py3-none-any.whl (718 kB)
[K     |████████████████████████████████| 718 kB 5.3 MB/s 
[?25hCollecting pyro-api&gt;=0.1.1
  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)
Requirement already satisfied: tqdm&gt;=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.64.0)
Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)
Collecting torch&gt;=1.11.0
  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)
[K     |████████████████████████████████| 750.6 MB 12 kB/s 
[?25hRequirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.21.6)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.11.0-&gt;pyro-ppl) (4.1.1)
Installing collected packages: torch, pyro-api, pyro-ppl
  Attempting uninstall: torch
    Found existing installation: torch 1.10.0+cu111
    Uninstalling torch-1.10.0+cu111:
      Successfully uninstalled torch-1.10.0+cu111
[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.
torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.
torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.[0m
Successfully installed pyro-api-0.1.2 pyro-ppl-1.8.1 torch-1.11.0
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pyro</span>
<span class="kn">import</span> <span class="n">pyro.distributions</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="n">pyro.nn</span> <span class="kn">import</span> <span class="n">PyroModule</span><span class="p">,</span> <span class="n">PyroSample</span>
<span class="kn">from</span> <span class="n">pyro.infer</span> <span class="kn">import</span> <span class="n">Predictive</span>
<span class="kn">from</span> <span class="n">pyro.infer</span> <span class="kn">import</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">from</span> <span class="n">pyro.infer.autoguide</span> <span class="kn">import</span> <span class="n">AutoDiagonalNormal</span>
<span class="kn">from</span> <span class="n">pyro.distributions</span> <span class="kn">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="n">torch.nn.functional</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">trange</span><span class="p">,</span> <span class="n">tqdm</span>
</code></pre></div></div> <p>Now let’s build our BNN!</p> <p>As a backbone use the MLP architecture introduced in the beginning of the notebook. However, because we will implement a custom <em>guide()</em>, define every layer explicitly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">My_MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">in_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">out_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">hid_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span> <span class="c1"># could also be nn.ReLU()
</span>
        <span class="c1">#3 hidden layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1">#hid --&gt; output prob
</span>        <span class="n">yhat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">yhat</span>
</code></pre></div></div> <p>Initialize the network. You will have to access it’s layers in your model and guide functions</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nc">My_MLP</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#confirm your layer names
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">net</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fc1.weight
fc1.bias
fc2.weight
fc2.bias
fc3.weight
fc3.bias
out.weight
out.bias
</code></pre></div></div> <p>Define the model:</p> <blockquote> <p>Probabilistic models in Pyro are specified as <em>model()</em> functions. This function defines how the output data is generated. Within the model() function, first, the pyro module <em>random_module()</em> converts the paramaters of our NN into random variables that have prior probability distributions. Second, in pyro <em>sample</em> we define that the output of the network is categorical, while the pyro <em>plate</em> allows us to vectorize this function for computational efficiency.</p> </blockquote> <blockquote> <p>Hint: remember we are doing a classification instead of regression!</p> </blockquote> <p>We will ‘cheat’ a little: to speed up training and limit a bit more the number of paramters we need to optimize, we will implement a BNN where only the <strong>last layer</strong> is Bayesian!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#if you want to try later a full BNN, you can uncomment the other lines
</span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>

    <span class="c1">#weight and bias priors
</span>
    <span class="c1"># fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight)).to_event(2)
</span>    <span class="c1"># fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias)).to_event(1)
</span>
    <span class="c1"># fc2w_prior = Normal(loc=torch.zeros_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight)).to_event(2)
</span>    <span class="c1"># fc2b_prior = Normal(loc=torch.zeros_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias)).to_event(1)
</span>
    <span class="c1"># fc3w_prior = Normal(loc=torch.zeros_like(net.fc3.weight), scale=torch.ones_like(net.fc3.weight)).to_event(2)
</span>    <span class="c1"># fc3b_prior = Normal(loc=torch.zeros_like(net.fc3.bias), scale=torch.ones_like(net.fc3.bias)).to_event(1)
</span>    
    <span class="n">outw_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">)).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">outb_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">)).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
              <span class="c1"># 'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,
</span>              <span class="c1"># 'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior,
</span>              <span class="c1"># 'fc3.weight': fc3w_prior, 'fc3.bias': fc3b_prior,      
</span>              <span class="sh">'</span><span class="s">out.weight</span><span class="sh">'</span><span class="p">:</span> <span class="n">outw_prior</span><span class="p">,</span> <span class="sh">'</span><span class="s">out.bias</span><span class="sh">'</span><span class="p">:</span> <span class="n">outb_prior</span><span class="p">}</span>

    <span class="c1"># lift module parameters to random variables sampled from the priors
</span>    <span class="n">lifted_module</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">random_module</span><span class="p">(</span><span class="sh">"</span><span class="s">module</span><span class="sh">"</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">priors</span><span class="p">)</span>
    <span class="c1"># sample a regressor (which also samples w and b)
</span>    <span class="n">lifted_reg_model</span> <span class="o">=</span> <span class="nf">lifted_module</span><span class="p">()</span>
    
    
    <span class="k">with</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">plate</span><span class="p">(</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span> 
        <span class="n">yhat</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">lifted_reg_model</span><span class="p">(</span><span class="n">x_data</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="sh">"</span><span class="s">obs</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">yhat</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">y_data</span><span class="p">)</span>
</code></pre></div></div> <p>implement the guide(), <em>variational distribution</em>:</p> <blockquote> <p>the guide allows us to initialise a well behaved distribution which later we can optimize to approximate the true posterior</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">softplus</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Softplus</span><span class="p">()</span>

<span class="c1">#similarly here, if you want a full BNN uncomment the other layers
</span><span class="k">def</span> <span class="nf">my_guide</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>
  
    <span class="c1"># # First layer weight distribution priors
</span>    <span class="c1"># fc0w_mu_param = pyro.param("fc0w_mu", torch.randn_like(net.fc1.weight))
</span>    <span class="c1"># fc0w_sigma_param = softplus(pyro.param("fc0w_sigma", torch.randn_like(net.fc1.weight)))
</span>    <span class="c1"># fc0w_prior = Normal(loc=fc0w_mu_param, scale=fc0w_sigma_param).to_event(2)
</span>    <span class="c1"># # First layer bias distribution priors
</span>    <span class="c1"># fc0b_mu_param = pyro.param("fc0b_mu", torch.randn_like(net.fc1.bias))
</span>    <span class="c1"># fc0b_sigma_param = softplus(pyro.param("fc0b_sigma", torch.randn_like(net.fc1.bias)))
</span>    <span class="c1"># fc0b_prior = Normal(loc=fc0b_mu_param, scale=fc0b_sigma_param).to_event(1)
</span>
    <span class="c1"># # Second layer weight distribution priors
</span>    <span class="c1"># fc1w_mu_param = pyro.param("fc1w_mu", torch.randn_like(net.fc2.weight))
</span>    <span class="c1"># fc1w_sigma_param = softplus(pyro.param("fc1w_sigma", torch.randn_like(net.fc2.weight)))
</span>    <span class="c1"># fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param).to_event(2)
</span>    <span class="c1"># # Second layer bias distribution priors
</span>    <span class="c1"># fc1b_mu_param = pyro.param("fc1b_mu", torch.randn_like(net.fc2.bias))
</span>    <span class="c1"># fc1b_sigma_param = softplus(pyro.param("fc1b_sigma", torch.randn_like(net.fc2.bias)))
</span>    <span class="c1"># fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param).to_event(1)
</span>
    <span class="c1"># # Third layer weight distribution priors
</span>    <span class="c1"># fc2w_mu_param = pyro.param("fc2w_mu", torch.randn_like(net.fc3.weight))
</span>    <span class="c1"># fc2w_sigma_param = softplus(pyro.param("fc2w_sigma", torch.randn_like(net.fc3.weight)))
</span>    <span class="c1"># fc2w_prior = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param).to_event(2)
</span>    <span class="c1"># # Third layer bias distribution priors
</span>    <span class="c1"># fc2b_mu_param = pyro.param("fc2b_mu", torch.randn_like(net.fc3.bias))
</span>    <span class="c1"># fc2b_sigma_param = softplus(pyro.param("fc2b_sigma", torch.randn_like(net.fc3.bias)))
</span>    <span class="c1"># fc2b_prior = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param).to_event(1)
</span>

    <span class="c1"># Output layer weight distribution priors
</span>    <span class="n">outw_mu_param</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outw_mu</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">))</span>
    <span class="n">outw_sigma_param</span> <span class="o">=</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outw_sigma</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">)))</span>
    <span class="n">outw_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">outw_mu_param</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">outw_sigma_param</span><span class="p">).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Output layer bias distribution priors
</span>    <span class="n">outb_mu_param</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outb_mu</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">))</span>
    <span class="n">outb_sigma_param</span> <span class="o">=</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outb_sigma</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">)))</span>
    <span class="n">outb_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">outb_mu_param</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">outb_sigma_param</span><span class="p">).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
              <span class="c1"># 'fc1.weight': fc0w_prior, 'fc1.bias': fc0b_prior,
</span>              <span class="c1"># 'fc2.weight': fc1w_prior, 'fc2.bias': fc1b_prior, 
</span>              <span class="c1"># 'fc3.weight': fc2w_prior, 'fc3.bias': fc2b_prior, 
</span>              <span class="sh">'</span><span class="s">out.weight</span><span class="sh">'</span><span class="p">:</span> <span class="n">outw_prior</span><span class="p">,</span> <span class="sh">'</span><span class="s">out.bias</span><span class="sh">'</span><span class="p">:</span> <span class="n">outb_prior</span><span class="p">}</span>
    
    <span class="n">lifted_module</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">random_module</span><span class="p">(</span><span class="sh">"</span><span class="s">module</span><span class="sh">"</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">priors</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">lifted_module</span><span class="p">()</span>

</code></pre></div></div> <p>Initialize the stochastic variational inference (SVI)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">adam</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">({</span><span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">})</span>
<span class="n">svi</span> <span class="o">=</span> <span class="nc">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">my_guide</span><span class="p">,</span> <span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="nc">Trace_ELBO</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pyro</span><span class="p">.</span><span class="nf">clear_param_store</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">svi</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">])</span>
    <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]


/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:495: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.
  FutureWarning,
</code></pre></div></div> <p><strong>Test</strong></p> <p>We are going to use our learned <em>guide()</em> function to do predictions. Why? Because the <em>model()</em> function knows the <strong>priors</strong> for the weights and biases, <strong>not</strong> the learned posterior. The <em>guide()</em> contains the approximate posterior distributions of the parameter values, which we want to use to make the predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#we sample new sets of weights and parameters 10 times given by num_samples
</span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sampled_models</span> <span class="o">=</span> <span class="p">[</span><span class="nf">my_guide</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)]</span>
    <span class="n">yhats</span> <span class="o">=</span> <span class="p">[</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">data</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">sampled_models</span><span class="p">]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">yhats</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span>

<span class="n">test_preds</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:495: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.
  FutureWarning,


Test accuracy is 91.63%
</code></pre></div></div> <p><strong>Evaluate on rotated images</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">def</span> <span class="nf">predict_probability</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sampled_models</span> <span class="o">=</span> <span class="p">[</span><span class="nf">my_guide</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)]</span>
    <span class="n">yhats</span> <span class="o">=</span> <span class="p">[</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">sampled_models</span><span class="p">]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">yhats</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span>

<span class="n">bnn_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="c1"># compute predictions
</span>  <span class="n">bnn_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">predict_probability</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:495: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.
  FutureWarning,
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot Ensemble results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, BNN</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, BNN</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_123_0.png" alt="png"></p> <h3 id="show-entropies-for-all-four-models">Show entropies for all four models.</h3> <p>Which is the best at detecting the distribution shift?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot MC dropout results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, MC Dropout</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, MC Dropout</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot Ensemble results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot BNN results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, BNN</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, BNN</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_125_0.png" alt="png"></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Bharat Vyas. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>