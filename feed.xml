<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://vyasb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vyasb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-01T18:08:28+00:00</updated><id>https://vyasb.github.io/feed.xml</id><title type="html">blank</title><subtitle>A whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Motion Capture - How does it work?</title><link href="https://vyasb.github.io/blog/2023/motion-capture/" rel="alternate" type="text/html" title="Motion Capture - How does it work?"/><published>2023-03-24T00:00:00+00:00</published><updated>2023-03-24T00:00:00+00:00</updated><id>https://vyasb.github.io/blog/2023/motion-capture</id><content type="html" xml:base="https://vyasb.github.io/blog/2023/motion-capture/"><![CDATA[<h3 id="motion-capture-technology-how-does-it-work">Motion Capture Technology: How Does It Work?</h3> <p>Motion capture, often abbreviated as mocap, is a fascinating technology that bridges the gap between the physical and digital worlds. It captures the movement of objects or people and translates that data into digital form. This technology is instrumental in various fields, including film production, video game development, sports analysis, and medical applications. This article delves into the intricacies of motion capture technology, exploring its history, how it works, its applications, and the future of this cutting-edge technology.</p> <h4 id="a-brief-history-of-motion-capture">A Brief History of Motion Capture</h4> <p>Motion capture technology has its roots in the late 19th and early 20th centuries. The earliest attempts to capture motion involved using still photography. Eadweard Muybridge, an English photographer, is famous for his sequential photographs of a galloping horse, which laid the groundwork for motion analysis.</p> <p>The first significant leap in motion capture technology occurred in the 1970s with the advent of digital computers. Pioneers like Max Fleischer and Lee Harrison III experimented with devices that could capture and analyze human movement. However, it wasn’t until the 1980s and 1990s that mocap began to take its modern form, thanks to advancements in computer graphics and sensor technology.</p> <h4 id="how-motion-capture-works">How Motion Capture Works</h4> <p>Motion capture technology can be broadly categorized into three types: optical, non-optical, and inertial systems. Each type uses different methodologies and equipment to capture motion data.</p> <h5 id="optical-systems">Optical Systems</h5> <p>Optical motion capture systems are the most widely used and rely on cameras and reflective markers or LED markers. Here’s a breakdown of the process:</p> <ol> <li> <p><strong>Preparation</strong>: The subject (actor or object) wears a suit with reflective markers or LED lights placed at key points on their body. These markers are positioned at joints and other significant locations to capture precise movements.</p> </li> <li> <p><strong>Capture</strong>: Multiple cameras are set up around the capture area. These cameras are equipped with infrared sensors to detect the markers. When the subject moves, the cameras track the markers’ positions in three-dimensional space.</p> </li> <li> <p><strong>Data Processing</strong>: The data from the cameras are sent to a computer system, which triangulates the marker positions to create a 3D model of the movement. Specialized software then processes this data to remove any noise and create a smooth animation.</p> </li> <li> <p><strong>Application</strong>: The captured motion data can then be applied to a digital character or model in a virtual environment. This process is known as “retargeting.”</p> </li> </ol> <p>Optical systems are known for their high accuracy and are commonly used in film production and high-end video game development. However, they require a controlled environment and can be expensive.</p> <h5 id="non-optical-systems">Non-Optical Systems</h5> <p>Non-optical systems use magnetic, mechanical, or acoustic sensors to capture motion. These systems do not rely on cameras and can be more versatile in certain environments.</p> <ol> <li> <p><strong>Magnetic Systems</strong>: These systems use sensors that detect changes in a magnetic field generated by a transmitter. The sensors are placed on the subject, and their positions are tracked based on the magnetic field’s variations. Magnetic systems can work in environments where optical systems might struggle, but they are susceptible to magnetic interference.</p> </li> <li> <p><strong>Mechanical Systems</strong>: Mechanical systems use exoskeletons or suits with built-in sensors that measure joint angles and limb positions. These systems are less affected by environmental factors and can capture motion in real-time without the need for line-of-sight.</p> </li> <li> <p><strong>Acoustic Systems</strong>: Acoustic motion capture systems use sound waves to determine the position of markers. Microphones placed around the capture area detect the sound emitted by markers, and the system triangulates their positions. Acoustic systems are less common due to their susceptibility to noise interference.</p> </li> </ol> <h5 id="inertial-systems">Inertial Systems</h5> <p>Inertial motion capture systems use accelerometers and gyroscopes to measure movement. These sensors are often embedded in suits or wearable devices. Inertial systems offer several advantages:</p> <ol> <li> <p><strong>Portability</strong>: Inertial systems are highly portable and do not require a controlled environment. This makes them ideal for capturing motion outdoors or in unconventional settings.</p> </li> <li> <p><strong>Real-Time Capture</strong>: These systems can capture motion in real-time, making them useful for applications that require immediate feedback, such as sports analysis or live performances.</p> </li> <li> <p><strong>Scalability</strong>: Inertial systems can be scaled to capture multiple subjects simultaneously without the need for complex setups.</p> </li> </ol> <p>However, inertial systems can suffer from drift over time, where the accuracy of the captured data degrades. Advanced algorithms and calibration techniques are used to mitigate this issue.</p> <h4 id="applications-of-motion-capture-technology">Applications of Motion Capture Technology</h4> <p>Motion capture technology has revolutionized several industries by enabling more realistic and precise motion analysis and reproduction. Here are some of the key applications:</p> <h5 id="film-and-animation">Film and Animation</h5> <p>One of the most well-known uses of motion capture is in the film and animation industry. Motion capture allows filmmakers to create realistic and lifelike animations by capturing the nuanced movements of actors. Notable examples include the creation of Gollum in “The Lord of the Rings” series and the Na’vi characters in “Avatar.” Motion capture also helps reduce the time and cost associated with traditional animation techniques.</p> <h5 id="video-games">Video Games</h5> <p>The video game industry has embraced motion capture to enhance the realism of character animations. Games like “The Last of Us,” “Red Dead Redemption 2,” and the “FIFA” series use motion capture to create fluid and lifelike movements. This technology allows developers to create more immersive gaming experiences by accurately replicating human motion.</p> <h5 id="sports-and-biomechanics">Sports and Biomechanics</h5> <p>Motion capture is extensively used in sports to analyze athletes’ movements and improve performance. Coaches and trainers can use motion capture data to identify inefficiencies in an athlete’s technique and develop targeted training programs. Additionally, motion capture is used in biomechanics research to study human movement and develop better prosthetics and rehabilitation techniques.</p> <h5 id="medical-applications">Medical Applications</h5> <p>In the medical field, motion capture technology is used for gait analysis, rehabilitation, and surgery planning. Gait analysis involves studying patients’ walking patterns to diagnose and treat conditions such as cerebral palsy or stroke. Motion capture can also aid in designing personalized rehabilitation programs and assist in planning complex surgical procedures by providing detailed anatomical models.</p> <h5 id="virtual-reality-and-augmented-reality">Virtual Reality and Augmented Reality</h5> <p>Motion capture plays a crucial role in virtual reality (VR) and augmented reality (AR) applications. In VR, motion capture enables users to interact with virtual environments in a natural and intuitive manner. In AR, motion capture can enhance real-world experiences by overlaying digital content onto the physical world. This technology is used in various applications, from gaming and entertainment to training simulations and remote collaboration.</p> <h4 id="the-future-of-motion-capture-technology">The Future of Motion Capture Technology</h4> <p>The future of motion capture technology is promising, with ongoing advancements aimed at improving accuracy, affordability, and ease of use. Here are some trends and innovations shaping the future of mocap:</p> <h5 id="real-time-motion-capture">Real-Time Motion Capture</h5> <p>Real-time motion capture is becoming increasingly important, especially in live performances and interactive applications. Advances in processing power and sensor technology are enabling real-time capture and rendering of complex movements, allowing for more dynamic and interactive experiences.</p> <h5 id="machine-learning-and-ai">Machine Learning and AI</h5> <p>Integrating machine learning and artificial intelligence with motion capture systems is opening new possibilities. AI algorithms can enhance motion capture accuracy by predicting and correcting errors in real-time. Machine learning can also help create more realistic animations by analyzing vast amounts of motion data and identifying patterns.</p> <h5 id="wearable-and-wireless-systems">Wearable and Wireless Systems</h5> <p>The development of wearable and wireless motion capture systems is making the technology more accessible and versatile. These systems eliminate the need for extensive setups and allow for more natural movement, expanding the range of possible applications.</p> <h5 id="improved-affordability">Improved Affordability</h5> <p>As technology advances, the cost of motion capture systems is expected to decrease. This will make motion capture more accessible to smaller studios, independent filmmakers, and researchers, democratizing the technology and fostering innovation.</p> <h5 id="expanded-applications">Expanded Applications</h5> <p>Motion capture technology will continue to find new applications in emerging fields. For example, motion capture could play a significant role in the development of autonomous robots and drones by providing precise movement data for training algorithms. Additionally, motion capture could be used in telemedicine to enable remote diagnosis and treatment.</p> <h4 id="conclusion">Conclusion</h4> <p>Motion capture technology has come a long way since its inception, evolving into a sophisticated tool that bridges the gap between the physical and digital worlds. Its applications are vast and varied, impacting industries from entertainment to medicine. As technology continues to advance, motion capture will undoubtedly play an increasingly important role in shaping the future of human-computer interaction and digital content creation.</p> <p>Whether it’s bringing fantastical creatures to life on the big screen, enhancing the realism of video game characters, or improving the performance of athletes, motion capture technology is a testament to the incredible possibilities that arise when technology and creativity intersect. As we look to the future, the potential of motion capture is limited only by our imagination.</p>]]></content><author><name></name></author><category term="mocap,"/><category term="animation"/><summary type="html"><![CDATA[A deep dive into the motion capture technology and its use in animation & graphics pipeline]]></summary></entry><entry><title type="html">Understanding the ‘Understanding Diffusion Models- A Unified Perspective’ paper</title><link href="https://vyasb.github.io/blog/2023/intro-diffusion-models/" rel="alternate" type="text/html" title="Understanding the ‘Understanding Diffusion Models- A Unified Perspective’ paper"/><published>2023-03-12T00:00:00+00:00</published><updated>2023-03-12T00:00:00+00:00</updated><id>https://vyasb.github.io/blog/2023/intro-diffusion-models</id><content type="html" xml:base="https://vyasb.github.io/blog/2023/intro-diffusion-models/"><![CDATA[<h3 id="understanding-diffusion-models-a-unified-perspective">Understanding Diffusion Models: A Unified Perspective</h3> <p>This is just me reading this paper and making some notes on the go. Let me know if someone wants to discuss the paper or find something intersting in my notes. Pardon my casual poor language skills throughout 😄</p> <blockquote> <p>I will try to go through some concepts in detail, while we also talk briefly through other relevant concepts. The text is purely my interpretation of the published work by Calvin Luo, and please dont use these notes to understand the paper. Rather think of this text as an exercise to check if you understood the concepts better than me or not 😉</p> </blockquote> <ul> <li> <p>As I mentioned , these are just some notes. I’d suggest read the <a href="https://arxiv.org/abs/2208.11970">Paper</a> first.</p> </li> <li> <p>I wont go in detail of many concepts but sure you can.</p> </li> <li> <p>Please let me know if you find any error in my understanding of concepts. Always feel great to be wrong 😭</p> </li> </ul> <h2 id="introduction-"><font size="6" color="blue">Introduction </font></h2> <p>The goal of generative model is to learn a true data distribution <code class="language-plaintext highlighter-rouge">p(x)</code> given some observed samples <code class="language-plaintext highlighter-rouge">x</code>. The learned model can:</p> <ul> <li>Generate new samples from approximate data</li> <li>Evaluate the likelihood of sampled data</li> </ul> <p>There are various directions for generative models as mentioned in text. Generative Adversarial Networks (GANs) learn to model sampling in an adversarial manner.</p> <blockquote> <p>Adversarial means two sides which oppose each other. GANs have generator and discriminator networks both of which are trained in adversarial manner. One network tries to generate new data and the other attempts to predict if the output is fake or real data.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Likelihood-based</code> generative models are another class where the model assign high likelihood to the observed data samples. These include - <em>Autoregressive models</em>, <em>Normalizing flows</em>, and <em>Variational Autoencoders (VAEs)</em>.</p> <blockquote> <p>An autoregressive (AR) model is a type of statistical model used for analyzing and predicting time series data. It determine the probabilistic correlation between elements in a sequence, and use the knowledge derived to guess the next element in an unknown sequence. For example, during training, an autoregressive model processes several English language sentences and identifies that the word “is” always follows the word “it” It then generates a new sequence that has “it is” together.</p> </blockquote> <blockquote> <p>Normalizing flows are like starting with a simple shape, such as a ball of clay, and transforming it into a complex shape by squeezing and stretching it multiple times. Each transformation is reversible, so we can always go back to the ball shape. This process helps us model and understand complex patterns in data by starting from something simple and making it more intricate step-by-step.</p> </blockquote> <p>Thereare other types of genrative modeling like <code class="language-plaintext highlighter-rouge">Energy based modeling</code>, where we energy function to learn the distribution. Related to this, <code class="language-plaintext highlighter-rouge">Score based modeling</code> learn the score of energy based model as a neural network. In this paper, the author is describing both likelihood-basedd and score-based interpretations - and lots of maths BTS!! 🤓</p> <h2 id="background-"><font size="6" color="blue">Background </font></h2> <h5 id="latent-variable-vs-platos-allegory-of-cave"><strong>Latent Variable v/s Plato’s Allegory of Cave</strong></h5> <p>Now this is something very interesting and intuitive interpretation of latent variable. For many modalities, the <em>latent</em> variable <code class="language-plaintext highlighter-rouge">z</code> is an unseen random variable that is inferred from the observed data and is used to represent or generate the given data.</p> <h6 id="but-what-is-the-paltos-allegory-of-cave-and-how-is-it-related-to-latent-variable">But what is the Palto’s allegory of cave and how is it related to latent variable?</h6> <p>In the Allegory, a group of people are chained inside a cave their whole life and can only see 2D shadows in front of them whch are generated by 3D objects (most probably some creatures or animals) passed before a fire. Now, everything they observe is a projection of actual objects which they never saw and would never see in future. To such people, they observe things by some high-dimensional abstarct concepts like- how far the shadow goes, speed of movement, or so on, but quite different concepts than normal people for sure. Since, the cave people can never see (or fully comprehend) the hidden objects they can still reason or infer about the objects using their own concepts. This reminds me of a very interesting reddit thread which could be a total BS but I enjoyed reading it <a href="https://www.reddit.com/r/AskScienceDiscussion/comments/7gvlhp/why_does_life_only_exist_in_three_dimensions/">thread link here</a>.</p> <p>Now analogously, the objects we see in everyday life can also be generated as function of some high-level representations - e.g., some absract properties like color, size, shape etc. Its just that our high-level representations are w.r.t 3D interpretation and cave people have the same for 2D shadows they see.</p> <p>The allegory illustrates the idea of latent variables as potentially unobservable representations that determine observations - sounds cool. But, (of course there is a but here) the idea also illustrates to learn a representation of higher dimension. In generative modeling, we try to learn a lower dimensional latent representation than the higehr ones. Think of it as a form of compression, and at the same time we try to uncover semantically meaningful structure describing observations. Also, if we were to learn high-dimensional representation then we would need very strong priors and things would become more complicated over time.</p> <p>So, this was the end of some cool analogy described in the paper. Its always nice to read some mix of philosophy and science. Now its time for some maths 😈</p> <h5 id="evidence-lower-bound"><strong>Evidence Lower Bound</strong></h5> <p>We start with the latent variable \(z\) and data we observe with the joint distribution \(p(x, z)\)</p> <p>In the “Likelihood based” generative modeling, we want to maximize the \(p(x)\) of all observed $ x $. There are two ways we could manipulate the joitn distribution to recover the likelihood of purely observed data \(p(x)\).</p> <p>The first one is: we explicitly marginalize out the latent variable $ z $. The marginal distribution is obtained by integrating \(p(x, z)\) with respect to \(z\).</p> \[p(x) = \int p(x, z) \, dz\] <p>Here, out latent variable \(z\) is a continous varibale, and we want to sum over all the possible values of latent variable (becasue \(x\) can occur with any value of \(z\))</p>]]></content><author><name></name></author><category term="diffusionModel,"/><category term="generativeModel"/><summary type="html"><![CDATA[Notes on the paper, and introduction to diffusion models]]></summary></entry><entry><title type="html">Exploring Bayesian Neural Networks with Pyro</title><link href="https://vyasb.github.io/blog/2022/bayesian-nn/" rel="alternate" type="text/html" title="Exploring Bayesian Neural Networks with Pyro"/><published>2022-03-05T00:00:00+00:00</published><updated>2022-03-05T00:00:00+00:00</updated><id>https://vyasb.github.io/blog/2022/bayesian-nn</id><content type="html" xml:base="https://vyasb.github.io/blog/2022/bayesian-nn/"><![CDATA[<h3 id="comparing-bnns-to-non-bayesian-methods-for-uncertainty-estimates">Comparing BNNs to Non-Bayesian Methods for Uncertainty Estimates</h3> <p><strong>Filled notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Complete_DLII_BNN_2_2.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" alt="View on Github"/></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Complete_DLII_BNN_2_2.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Collab"/></a> <br/> <strong>Empty notebook:</strong> <a href="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Student_DLII_BNN_2_2.ipynb"><img src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" alt="View Empty on Github"/></a> <a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Student_DLII_BNN_2_2.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open Empty In Collab"/></a> <br/> <strong>Authors:</strong> Ilze Amanda Auzina, Leonard Bereska and Eric Nalisnick</p> <p>In this tutorial we will investigate what are some benefits of Bayesian Neural Networks (BNN) over point estimate Neural Networks.</p> <p>Import the usual suspects…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">trange</span><span class="p">,</span> <span class="n">tqdm</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">81</span><span class="p">);</span>
</code></pre></div></div> <h3 id="simulate-data">Simulate Data</h3> <p>Let’s simulate a wiggly line and draw observations in separated regions…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set Numpy seed
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># sample observations
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(.</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)])</span>
<span class="n">ε</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ε</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ε</span><span class="p">))</span> <span class="o">+</span> <span class="n">ε</span>

<span class="c1"># compute true function
</span><span class="n">x_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">x_true</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_true</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_true</span><span class="p">)</span>
 


<span class="c1"># make plot
</span><span class="n">xlims</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="n">ylims</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_7_0.png" alt="png"/></p> <p>As you can see, we have the true function in blue. The observations are observable in two regions of the function and there is some noise in their measurement. We will use this simple data to showcase the differences between BNN and deterministic NN.</p> <h3 id="define-non-bayesian-neural-network">Define non-Bayesian Neural Network</h3> <p>First let’s create our point estimate neural network, in other words a standard fully connected MLP. We will define the number of hidden layers dynamically so we can reuse the same class for different depths. We will also add two flags, <em>residual</em> and <em>dropout</em> flag, this will allow us to easily use the same architecture for our BNN.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout_prob</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">in_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">out_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">hid_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">n_hid_layers</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span> <span class="c1"># could also be nn.ReLU()
</span>    <span class="n">self</span><span class="p">.</span><span class="n">residual_flag</span> <span class="o">=</span> <span class="n">residual_flag</span>
    <span class="n">self</span><span class="p">.</span><span class="n">dropout_flag</span> <span class="o">=</span> <span class="n">dropout_flag</span>

    <span class="c1"># optional: activate dropout
</span>    <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>
        
    <span class="c1"># dynamically define architecture
</span>    <span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_hid_layers</span> <span class="o">*</span> <span class="p">[</span><span class="n">hid_dim</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">out_dim</span><span class="p">]</span>
    <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span><span class="p">))]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span><span class="n">layer_list</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1">#x = x.reshape(-1, 1)
</span>    <span class="c1"># input --&gt; hid 
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
      <span class="c1"># hid --&gt; hid
</span>      <span class="n">x_temp</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

      <span class="c1"># optional: compute dropout mask
</span>      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout_flag</span><span class="p">:</span>
        <span class="n">x_temp</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x_temp</span><span class="p">)</span>

      <span class="c1"># optional: compute residual connection
</span>      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">residual_flag</span><span class="p">:</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_temp</span> <span class="o">+</span> <span class="n">x</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_temp</span>


    <span class="c1"># hid --&gt; output mean
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu</span>

</code></pre></div></div> <h3 id="train-one-deterministic-nn">Train one deterministic NN</h3> <p><strong>Training</strong></p> <p>Now let’s train our MLP with the training data we generated above:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define model and data
</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">hid_dim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">float</span><span class="p">()[:,</span> <span class="bp">None</span><span class="p">]</span> 
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#select an optimizer and loss function
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="c1"># training loop
</span><span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
  <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Evaluate</strong></p> <p>Let’s investigate how our deterministic MLP generalizes over the entire domain of our input variable $x$ (the model was only trained on the observations, now we will also pass in data outside this region)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute predictions everywhere
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">xlims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3000</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> 
<span class="n">y_preds</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">();</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">mlp function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_18_0.png" alt="png"/></p> <p>We can see that our deterministic MLP (red line) has correctly learned the data distribution in the training regions, however, as the model has not learned the underlying sinusoidal wave function, it’s predictions outside the training region are inaccurate. As our MLP is a point estimate NN we have no measure confidence in the predictions outside the training region. In the upcoming sections let’s see how this compares to BNN.</p> <h3 id="deep-ensemble">Deep Ensemble</h3> <p>Deep ensembles were first introduced by <a href="https://arxiv.org/abs/1612.01474">Lakshminarayanan et al. (2017)</a>. As the name implies multiple point estimate NN are trained, <em>an ensemble</em>, and the final prediction is computed as an average across the models. From a Bayesian perspective the different point estimates correspond to modes of a Bayesian posterior. This can be interpreted as approximating the posterior with a distribution parametrized as multiple Dirac deltas:</p> <p>\(q_{\phi}(\theta | D) = \sum_{\theta_{i} ∈ ϕ} \alpha_{\theta_{i}} δ_{\theta_{i}}(\theta)\) where $\alpha_{\theta_{i}}$ are positive constants such that their sum is equal to one.</p> <p><strong>Training</strong></p> <p>We will reuse the MLP architecture introduced before, simply now we will train an ensemble of such models</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#define the size of the ensemble: number of NN to train
</span><span class="n">ensemble_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mlp_optimizers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># initialize the ensemble
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">hid_dim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">ensemble</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">mlp_optimizers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train each ensemble component
</span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>

  <span class="c1"># training loop for one NN
</span>  <span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
    <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="nf">net</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Evaluate</strong></p> <p>Same as before, let’s investigate how our Deep Ensemble performs on the entire data domain of our input variable $x$.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute predictions for each network
</span><span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
  <span class="n">y_preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">net</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> <span class="p">)</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_preds</span><span class="p">)</span>
</code></pre></div></div> <p>Plot each ensemble member’s predictive function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot each network's function
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_28_0.png" alt="png"/></p> <p>In this plot the benefit of an ensemble approach is not immediately clear. Still on the regions outside the training data each of the trained NN is inaccurate. So what is the benefit you might ask.</p> <p>Well let’s plot the above in a slightly different way: let’s visualize the ensemble’s <strong>uncertainty bands</strong>.</p> <blockquote> <table> <tbody> <tr> <td>From a Bayesian perspective we want to quantity the model’s uncertainty on its prediction. This is done via the marginal $p(y</td> <td>x, D)$, which can be computed as:</td> </tr> </tbody> </table> </blockquote> \[p(y|x, D) = \int_{\theta}p(y|x,\theta')p(\theta'|D)d\theta'\] <blockquote> <table> <tbody> <tr> <td>In practice, for Deep Ensembles we approximate the above by computing the mean and standard deviation across the ensemble. Meaning $p(\theta</td> <td>D)$ represents the parameters of one of the trained models, $\theta_{i} ∼ p(\theta</td> <td>D)$, which we then use to compute $y_{i} = f(x,\theta_{i})$, representing $p(y</td> <td>x,\theta’)$.</td> </tr> </tbody> </table> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute mean and standard deviation
</span><span class="n">y_mean</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">#408765</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">predictive mean</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">y_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#86cfac</span><span class="sh">'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_33_0.png" alt="png"/></p> <p>Now we see the benefit of a Bayesian approach. Outside the training region we not only have the point estimate, but also model’s uncertainty about it’s predicition.</p> <h3 id="monte-carlo-dropout">Monte Carlo Dropout</h3> <p>First we create our MC-Dropout Network. As you can see in the code below, creating a dropout network is extremely simple:</p> <blockquote> <p>we can reuse our existing network architecture, the only alteration is that during the forward pass we randomly <em>switch off</em> (zero) some of the elements of the input tensor</p> </blockquote> <table> <tbody> <tr> <td>The Bayesian interpretation of MC-Dropout is that we can see each dropout configuration as a different sample from the approximate posterior distribution $\theta_{i} ∼ q(\theta</td> <td>D)$.</td> </tr> </tbody> </table> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define MLP with dropout (probability of 0.5)
</span><span class="n">dropout_mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">hid_dim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training 
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
  <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Evaluate</strong></p> <p>Similarly to Deep Ensembles, we pass the test data multiple times through the MC-Dropout network. We do so to obtain $y_{i}$ at the different parameter settings, $\theta_{i}$ of the network, $y_{i}=f(x,\theta_{i})$, governed by the dropout mask.</p> <blockquote> <p>This is the main difference compared to dropout implementation in a deterministic NN where it serves as a regularization term. In normal dropout application during test time the dropout is <strong>not</strong> applied. Meaning that all connections are present, but the weights are <a href="https://cs231n.github.io/neural-networks-2/">adjusted</a></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_mc_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># compute predictions, resampling dropout mask for each forward pass
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_mc_samples</span><span class="p">):</span>
  <span class="n">y_preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> <span class="p">)</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_preds</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot of each dropout (sub)-model
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot each network's function
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_43_0.png" alt="png"/></p> <p>In the above plot each colored line (apart from blue) represents a different parametrization, $\theta_{i}$, of our MC-Dropout Network.</p> <p>Likewise to the Deep Ensemble Network, we can also compute the MC-dropout’s <strong>uncertainty bands</strong>.</p> <blockquote> <p>The approach in practice is the same as before: we compute the mean and standard deviation across each dropout mask, which corresponds to the marginal estimation we discussed earlier.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute mean and standard deviation
</span><span class="n">y_mean</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">y_preds</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">observations</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">#408765</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">predictive mean</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">y_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#86cfac</span><span class="sh">'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_47_0.png" alt="png"/></p> <p>In the same way as Deep Ensembles, MC-Dropout allows us to have an uncertainty estimate next to our point wise predictions. However, for the given use-case this has come with the cost of an overall drop in the model’s performance on the training regions. We observe this because at every pass through our network we randomly choose which nodes to keep, so one could argue that we hinder the networks optimal performance.</p> <h2 id="exercise-detecting-distribution-shift-on-mnist">Exercise: Detecting Distribution Shift on MNIST</h2> <p>In this exercise we will compare Bayesian NNs with deterministic NN on distribution shift detection task. To do this, we’ll monitor the predictive entropy as the distribution gradually shifts. A model with better uncertainty quantification should become less certain—that is, have a more entropic predictive distribuiton—as the input distribution shifts. Mathematically, our quantity of interest is: \(\mathbb{H}[y | x^{*}, D] = - \sum_{y} p(y | x^{*}, D) \log p(y | x^{*}, D)\) where $p(y | x^{*}, D)$ is the predictive distribuiton: \(p(y | x^{*}, D) = \int_{\theta} p(y | x^{*}, \theta) \ p(\theta | D) \ d \theta.\) The goal is to essentially replicate Figure #1 from the paper <a href="https://arxiv.org/abs/1603.04733">Multiplicative Normalizing Flows for Variational Bayesian Neural Networks</a>, comparing MC dropout, ensembles, and a Bayesian NN.</p> <p>We will be using the MNIST dataset, a set of 70,000 hand-written digit images, and we will generate a gradual distribution shift on the dataset by rotating the images. As such, the final plot will depict the change in the entropy of the predictive distribution (y-axis) as degree of rotation increases (x-axis). The paper above shows the result for one image. We, on the other hand, will average over multiple images to make a better comparison between models.</p> <p>First, let’s load MNIST…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># download MNIST
</span><span class="n">mnist_train</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">../data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">../data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz



  0%|          | 0/9912422 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz



  0%|          | 0/28881 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz



  0%|          | 0/1648877 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz



  0%|          | 0/4542 [00:00&lt;?, ?it/s]


Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw
</code></pre></div></div> <p>Visualize an image…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">img_idx</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">255</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_54_0.png" alt="png"/></p> <p>We’ll use rotation to simulate a smooth shift. Here’s how you can rotate a given image…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torchvision.transforms.functional</span> <span class="k">as</span> <span class="n">TF</span>

<span class="n">rotation_angle</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">rotated_image</span> <span class="o">=</span> <span class="n">TF</span><span class="p">.</span><span class="nf">rotate</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">img_idx</span><span class="p">][</span><span class="bp">None</span><span class="p">],</span> <span class="n">rotation_angle</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">rotated_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">255</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_56_0.png" alt="png"/></p> <p>Let’s setup the training data…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">.</span><span class="n">targets</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">.</span><span class="n">targets</span>
</code></pre></div></div> <p>Now that we have the data, let’s start training neural networks.</p> <h3 id="deterministic-network">Deterministic Network</h3> <p>We will reuse our MLP network architecture but increase the hidden layer dimensionality as well as the number of hidden layers:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training 
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">]))</span>
    <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span> <span class="c1">#x.shape[0]
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Test</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
  <span class="nf">return </span><span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="n">predictions</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">targets</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get test accuracy, to make sure we have a satisfactory model
</span><span class="n">mlp</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">test_preds</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test accuracy is 96.49%
</code></pre></div></div> <h3 id="rotating-the-images">Rotating the images</h3> <p>Now let’s compute predictive entropy on some rotated images…</p> <p>First we will generate the rotated images with an increasing rotation angle from the test images:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use angles from 0 to 90 degrees
</span><span class="n">rotation_angles</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">31</span><span class="p">)]</span>
<span class="n">n_test_images</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">rotated_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">angle</span> <span class="ow">in</span> <span class="n">rotation_angles</span><span class="p">:</span>
  <span class="n">rotated_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">TF</span><span class="p">.</span><span class="nf">rotate</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">.</span><span class="n">data</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">].</span><span class="nf">float</span><span class="p">()[</span><span class="bp">None</span><span class="p">],</span> <span class="n">angle</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_test_images</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div> <p>Evaluate the trained MLP on the rotated images:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">deter_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="n">deter_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">mlp</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
  <span class="nf">return </span><span class="p">(</span><span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Predictive Entropy</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_75_0.png" alt="png"/></p> <p>In the above plot we can see how well our deterministic MLP performs on the test images as the rotation angle is increased (increasing distribution shift). As expected, with a higher rotation angle model’s accuracy drops and its predictive entropy increases, as the network class predictions are more uniformly distributed (recall the predictive entropy equation)</p> <h3 id="monte-carlo-dropout-network">Monte Carlo Dropout Network</h3> <p>Let’s create our Dropout Network. We keep the network depth and hidden layer size the same as for the MLP for a fair model comparison</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dropout_mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training 
</span><span class="n">mlp_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">]))</span>
    <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">mlp_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Test</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get test accuracy
</span><span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">test_preds</span> <span class="o">=</span> <span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test accuracy is 96.57%
</code></pre></div></div> <p><strong>Evaluate on rotated images</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_mc_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dropout_mlp</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span> <span class="c1">#we set the model to train so the dropout layer is 'active'
</span>
<span class="n">dropout_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="c1"># compute predictions, resampling dropout mask for each forward pass
</span>  <span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_test_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_mc_samples</span><span class="p">):</span>
    <span class="n">y_preds</span> <span class="o">+=</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">dropout_mlp</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">dropout_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">y_preds</span> <span class="o">/</span> <span class="n">n_mc_samples</span> <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot MC dropout results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, MC Dropout</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, MC Dropout</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_86_0.png" alt="png"/></p> <p>The accuracy is on par between the MC-Dropout and MLP. However, now we can see that the MC-Dropout has a better uncertainty quantification as it’s predictive entropy is higher. What this means is that overall both model prediction uncertainty increases as the distribution shift increases, however, as MC-dropout computes a model average it is more sensitive (more uncertain) about the predictions with a higher rotation angle.</p> <h3 id="deep-ensemble-1">Deep Ensemble</h3> <p>Now let’s investigate Deep Ensemble performance. We will use the exact same network hyperparameters as for the MLP:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define the size of the ensemble: number of NN to train
</span><span class="n">ensemble_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mlp_optimizers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># initialize the ensemble
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ensemble_size</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">residual_flag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout_flag</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">ensemble</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">mlp_optimizers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train each ensemble component
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">mlp_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
  <span class="c1"># training loop for one NN
</span>  <span class="n">net</span> <span class="o">=</span> <span class="n">ensemble</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
      <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
      <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="nf">mlp_criterion</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="nf">net</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">]))</span>
      <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
      <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
      <span class="n">mlp_optimizers</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">step</span><span class="p">()</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]



  0%|          | 0/30 [00:00&lt;?, ?it/s]
</code></pre></div></div> <p><strong>Test</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get test accuracy
# compute predictions for each network
</span><span class="n">test_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
  <span class="n">net</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
  <span class="n">test_preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">net</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">test_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">test_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">test_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">test_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="n">ensemble_size</span> <span class="c1">#we average the predictions over the ensemble
</span>
<span class="c1"># compute the accuracy
</span><span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">.</span><span class="nf">int</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test accuracy is 93.88%
</code></pre></div></div> <p><strong>Evaluate on rotated images</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ensemble_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="c1"># compute predictions for each ensemble network
</span>  <span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_test_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">net</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ensemble</span><span class="p">):</span>
    <span class="n">y_preds</span> <span class="o">+=</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">net</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ensemble_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">y_preds</span> <span class="o">/</span> <span class="n">ensemble_size</span> <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot Ensemble results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_97_0.png" alt="png"/></p> <table> <tbody> <tr> <td>The accuracy of the Deep Ensemble is initially lower than of our deterministic MLP. However, this is accounted by the fact that the Deep Ensemble is better in uncertainty quantification as it’s predictive entropy is higher. How to interpret this? Well, in Deep Ensemble we have trained multiple deterministic MLPs. As the rotation angle increases the point-estimates of each model become more varied, hence the average of these point estimates, $p(y</td> <td>x^{*},D)$, has a more uniform distribution across the classes. As such, the resulting predictive entorpy is higher, as the log of $p(y</td> <td>x^{*},D)$ is more negative for smaller values (recall <a href="https://upload.wikimedia.org/wikipedia/commons/8/81/Logarithm_plots.png">log function</a>).</td> </tr> </tbody> </table> <h3 id="bayesian-neural-network">Bayesian Neural Network</h3> <p>We will train a <em>truly</em> Bayesian Neural Netowrk with SVI</p> <p>First, lets install the pyro package:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">pyro</span><span class="o">-</span><span class="n">ppl</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Collecting pyro-ppl
  Downloading pyro_ppl-1.8.1-py3-none-any.whl (718 kB)
[K     |████████████████████████████████| 718 kB 5.3 MB/s 
[?25hCollecting pyro-api&gt;=0.1.1
  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)
Requirement already satisfied: tqdm&gt;=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.64.0)
Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)
Collecting torch&gt;=1.11.0
  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)
[K     |████████████████████████████████| 750.6 MB 12 kB/s 
[?25hRequirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.21.6)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.11.0-&gt;pyro-ppl) (4.1.1)
Installing collected packages: torch, pyro-api, pyro-ppl
  Attempting uninstall: torch
    Found existing installation: torch 1.10.0+cu111
    Uninstalling torch-1.10.0+cu111:
      Successfully uninstalled torch-1.10.0+cu111
[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.
torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.
torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.[0m
Successfully installed pyro-api-0.1.2 pyro-ppl-1.8.1 torch-1.11.0
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pyro</span>
<span class="kn">import</span> <span class="n">pyro.distributions</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="n">pyro.nn</span> <span class="kn">import</span> <span class="n">PyroModule</span><span class="p">,</span> <span class="n">PyroSample</span>
<span class="kn">from</span> <span class="n">pyro.infer</span> <span class="kn">import</span> <span class="n">Predictive</span>
<span class="kn">from</span> <span class="n">pyro.infer</span> <span class="kn">import</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">from</span> <span class="n">pyro.infer.autoguide</span> <span class="kn">import</span> <span class="n">AutoDiagonalNormal</span>
<span class="kn">from</span> <span class="n">pyro.distributions</span> <span class="kn">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="n">torch.nn.functional</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">trange</span><span class="p">,</span> <span class="n">tqdm</span>
</code></pre></div></div> <p>Now let’s build our BNN!</p> <p>As a backbone use the MLP architecture introduced in the beginning of the notebook. However, because we will implement a custom <em>guide()</em>, define every layer explicitly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">My_MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">in_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">out_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">hid_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span> <span class="c1"># could also be nn.ReLU()
</span>
        <span class="c1">#3 hidden layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1">#hid --&gt; output prob
</span>        <span class="n">yhat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">yhat</span>
</code></pre></div></div> <p>Initialize the network. You will have to access it’s layers in your model and guide functions</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nc">My_MLP</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#confirm your layer names
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">net</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fc1.weight
fc1.bias
fc2.weight
fc2.bias
fc3.weight
fc3.bias
out.weight
out.bias
</code></pre></div></div> <p>Define the model:</p> <blockquote> <p>Probabilistic models in Pyro are specified as <em>model()</em> functions. This function defines how the output data is generated. Within the model() function, first, the pyro module <em>random_module()</em> converts the paramaters of our NN into random variables that have prior probability distributions. Second, in pyro <em>sample</em> we define that the output of the network is categorical, while the pyro <em>plate</em> allows us to vectorize this function for computational efficiency.</p> </blockquote> <blockquote> <p>Hint: remember we are doing a classification instead of regression!</p> </blockquote> <p>We will ‘cheat’ a little: to speed up training and limit a bit more the number of paramters we need to optimize, we will implement a BNN where only the <strong>last layer</strong> is Bayesian!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#if you want to try later a full BNN, you can uncomment the other lines
</span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>

    <span class="c1">#weight and bias priors
</span>
    <span class="c1"># fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight)).to_event(2)
</span>    <span class="c1"># fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias)).to_event(1)
</span>
    <span class="c1"># fc2w_prior = Normal(loc=torch.zeros_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight)).to_event(2)
</span>    <span class="c1"># fc2b_prior = Normal(loc=torch.zeros_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias)).to_event(1)
</span>
    <span class="c1"># fc3w_prior = Normal(loc=torch.zeros_like(net.fc3.weight), scale=torch.ones_like(net.fc3.weight)).to_event(2)
</span>    <span class="c1"># fc3b_prior = Normal(loc=torch.zeros_like(net.fc3.bias), scale=torch.ones_like(net.fc3.bias)).to_event(1)
</span>    
    <span class="n">outw_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">)).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">outb_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">)).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
              <span class="c1"># 'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,
</span>              <span class="c1"># 'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior,
</span>              <span class="c1"># 'fc3.weight': fc3w_prior, 'fc3.bias': fc3b_prior,      
</span>              <span class="sh">'</span><span class="s">out.weight</span><span class="sh">'</span><span class="p">:</span> <span class="n">outw_prior</span><span class="p">,</span> <span class="sh">'</span><span class="s">out.bias</span><span class="sh">'</span><span class="p">:</span> <span class="n">outb_prior</span><span class="p">}</span>

    <span class="c1"># lift module parameters to random variables sampled from the priors
</span>    <span class="n">lifted_module</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">random_module</span><span class="p">(</span><span class="sh">"</span><span class="s">module</span><span class="sh">"</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">priors</span><span class="p">)</span>
    <span class="c1"># sample a regressor (which also samples w and b)
</span>    <span class="n">lifted_reg_model</span> <span class="o">=</span> <span class="nf">lifted_module</span><span class="p">()</span>
    
    
    <span class="k">with</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">plate</span><span class="p">(</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span> 
        <span class="n">yhat</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="nf">lifted_reg_model</span><span class="p">(</span><span class="n">x_data</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="sh">"</span><span class="s">obs</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">yhat</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">y_data</span><span class="p">)</span>
</code></pre></div></div> <p>implement the guide(), <em>variational distribution</em>:</p> <blockquote> <p>the guide allows us to initialise a well behaved distribution which later we can optimize to approximate the true posterior</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">softplus</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Softplus</span><span class="p">()</span>

<span class="c1">#similarly here, if you want a full BNN uncomment the other layers
</span><span class="k">def</span> <span class="nf">my_guide</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>
  
    <span class="c1"># # First layer weight distribution priors
</span>    <span class="c1"># fc0w_mu_param = pyro.param("fc0w_mu", torch.randn_like(net.fc1.weight))
</span>    <span class="c1"># fc0w_sigma_param = softplus(pyro.param("fc0w_sigma", torch.randn_like(net.fc1.weight)))
</span>    <span class="c1"># fc0w_prior = Normal(loc=fc0w_mu_param, scale=fc0w_sigma_param).to_event(2)
</span>    <span class="c1"># # First layer bias distribution priors
</span>    <span class="c1"># fc0b_mu_param = pyro.param("fc0b_mu", torch.randn_like(net.fc1.bias))
</span>    <span class="c1"># fc0b_sigma_param = softplus(pyro.param("fc0b_sigma", torch.randn_like(net.fc1.bias)))
</span>    <span class="c1"># fc0b_prior = Normal(loc=fc0b_mu_param, scale=fc0b_sigma_param).to_event(1)
</span>
    <span class="c1"># # Second layer weight distribution priors
</span>    <span class="c1"># fc1w_mu_param = pyro.param("fc1w_mu", torch.randn_like(net.fc2.weight))
</span>    <span class="c1"># fc1w_sigma_param = softplus(pyro.param("fc1w_sigma", torch.randn_like(net.fc2.weight)))
</span>    <span class="c1"># fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param).to_event(2)
</span>    <span class="c1"># # Second layer bias distribution priors
</span>    <span class="c1"># fc1b_mu_param = pyro.param("fc1b_mu", torch.randn_like(net.fc2.bias))
</span>    <span class="c1"># fc1b_sigma_param = softplus(pyro.param("fc1b_sigma", torch.randn_like(net.fc2.bias)))
</span>    <span class="c1"># fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param).to_event(1)
</span>
    <span class="c1"># # Third layer weight distribution priors
</span>    <span class="c1"># fc2w_mu_param = pyro.param("fc2w_mu", torch.randn_like(net.fc3.weight))
</span>    <span class="c1"># fc2w_sigma_param = softplus(pyro.param("fc2w_sigma", torch.randn_like(net.fc3.weight)))
</span>    <span class="c1"># fc2w_prior = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param).to_event(2)
</span>    <span class="c1"># # Third layer bias distribution priors
</span>    <span class="c1"># fc2b_mu_param = pyro.param("fc2b_mu", torch.randn_like(net.fc3.bias))
</span>    <span class="c1"># fc2b_sigma_param = softplus(pyro.param("fc2b_sigma", torch.randn_like(net.fc3.bias)))
</span>    <span class="c1"># fc2b_prior = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param).to_event(1)
</span>

    <span class="c1"># Output layer weight distribution priors
</span>    <span class="n">outw_mu_param</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outw_mu</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">))</span>
    <span class="n">outw_sigma_param</span> <span class="o">=</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outw_sigma</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">weight</span><span class="p">)))</span>
    <span class="n">outw_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">outw_mu_param</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">outw_sigma_param</span><span class="p">).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Output layer bias distribution priors
</span>    <span class="n">outb_mu_param</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outb_mu</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">))</span>
    <span class="n">outb_sigma_param</span> <span class="o">=</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">pyro</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span><span class="sh">"</span><span class="s">outb_sigma</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">out</span><span class="p">.</span><span class="n">bias</span><span class="p">)))</span>
    <span class="n">outb_prior</span> <span class="o">=</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">outb_mu_param</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">outb_sigma_param</span><span class="p">).</span><span class="nf">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
              <span class="c1"># 'fc1.weight': fc0w_prior, 'fc1.bias': fc0b_prior,
</span>              <span class="c1"># 'fc2.weight': fc1w_prior, 'fc2.bias': fc1b_prior, 
</span>              <span class="c1"># 'fc3.weight': fc2w_prior, 'fc3.bias': fc2b_prior, 
</span>              <span class="sh">'</span><span class="s">out.weight</span><span class="sh">'</span><span class="p">:</span> <span class="n">outw_prior</span><span class="p">,</span> <span class="sh">'</span><span class="s">out.bias</span><span class="sh">'</span><span class="p">:</span> <span class="n">outb_prior</span><span class="p">}</span>
    
    <span class="n">lifted_module</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="nf">random_module</span><span class="p">(</span><span class="sh">"</span><span class="s">module</span><span class="sh">"</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">priors</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">lifted_module</span><span class="p">()</span>

</code></pre></div></div> <p>Initialize the stochastic variational inference (SVI)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">adam</span> <span class="o">=</span> <span class="n">pyro</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">({</span><span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">})</span>
<span class="n">svi</span> <span class="o">=</span> <span class="nc">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">my_guide</span><span class="p">,</span> <span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="nc">Trace_ELBO</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pyro</span><span class="p">.</span><span class="nf">clear_param_store</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">bar</span> <span class="o">=</span> <span class="nf">trange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">bar</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
    <span class="n">batch_low</span><span class="p">,</span> <span class="n">batch_high</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">svi</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_low</span><span class="p">:</span><span class="n">batch_high</span><span class="p">])</span>
    <span class="n">bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">loss</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  0%|          | 0/30 [00:00&lt;?, ?it/s]


/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:495: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.
  FutureWarning,
</code></pre></div></div> <p><strong>Test</strong></p> <p>We are going to use our learned <em>guide()</em> function to do predictions. Why? Because the <em>model()</em> function knows the <strong>priors</strong> for the weights and biases, <strong>not</strong> the learned posterior. The <em>guide()</em> contains the approximate posterior distributions of the parameter values, which we want to use to make the predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#we sample new sets of weights and parameters 10 times given by num_samples
</span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sampled_models</span> <span class="o">=</span> <span class="p">[</span><span class="nf">my_guide</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)]</span>
    <span class="n">yhats</span> <span class="o">=</span> <span class="p">[</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">data</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">sampled_models</span><span class="p">]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">yhats</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span>

<span class="n">test_preds</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy is %.2f%%</span><span class="sh">"</span> <span class="o">%</span><span class="p">(</span><span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:495: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.
  FutureWarning,


Test accuracy is 91.63%
</code></pre></div></div> <p><strong>Evaluate on rotated images</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">def</span> <span class="nf">predict_probability</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sampled_models</span> <span class="o">=</span> <span class="p">[</span><span class="nf">my_guide</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)]</span>
    <span class="n">yhats</span> <span class="o">=</span> <span class="p">[</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">sampled_models</span><span class="p">]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">yhats</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span>

<span class="n">bnn_pred_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">rotated_images</span><span class="p">:</span>
  <span class="c1"># compute predictions
</span>  <span class="n">bnn_pred_means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">predict_probability</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.7/dist-packages/pyro/primitives.py:495: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.
  FutureWarning,
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot Ensemble results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, BNN</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, BNN</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_123_0.png" alt="png"/></p> <h3 id="show-entropies-for-all-four-models">Show entropies for all four models.</h3> <p>Which is the best at detecting the distribution shift?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate plot 
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Rotation Angle</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># plot deterministic results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deter.</span><span class="sh">"</span><span class="p">);</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">deter_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deter.</span><span class="sh">"</span><span class="p">);</span>

<span class="c1"># plot MC dropout results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, MC Dropout</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dropout_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, MC Dropout</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot Ensemble results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ensemble_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">g-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, Deep Ensemble</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># plot BNN results
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">n_test_images</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Acc, BNN</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rotation_angles</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">bnn_pred_means</span><span class="p">],</span> <span class="sh">'</span><span class="s">y-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Ent, BNN</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div></div> <p><img src="Complete_DLII_BNN_2_2ipynb_files/Complete_DLII_BNN_2_2ipynb_125_0.png" alt="png"/></p>]]></content><author><name></name></author><category term="bayesian"/><summary type="html"><![CDATA[notebook on BNN, part of UvA Deep Learning II course]]></summary></entry></feed>